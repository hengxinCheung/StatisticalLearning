k近邻法（k-nearest neighbor，k-NN）是一种基本分类与回归方法。这里值讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻法不具有显式的学习过程。k值选择、距离度量以及分类决策规则是k近邻法的三个基本要素。

## k近邻算法

k近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把这个输入实例分为这个类。

**k近邻法算法：**

输入：训练数据集 $$T = \{ (x_1,y_1), (x_2, y_2), .., (x_N,y_N) \}$$
输出：实例 $x$ 所属的类 $y$

- 根据给定的距离度量，在训练集 $T$ 中找出与 $x$ 最邻近的k个点，涵盖这k个点的 $x$ 的邻域记作 $N_k(x)$
- 在 $N_k(x)$ 中根据分类规则（如多数表决）决定 $x$ 的类别 $y$ ：$$y = argmax \sum_{x_i \in N_k(x) I(y_i = c_j)}$$

k近邻法的特殊情况是k=1的情形，称为最近邻算法。

## k近邻模型

k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素：距离度量，k值选择和分类决策规则决定。

### 模型

k近邻法中，当训练集，距离度量（如欧式距离），k值以及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间中每个点所属的类。

特征空间中，对每个训练实例点 $x_i$，距离该点比其他点更近的所有点组成一个区域，叫做单元(cell)。每个训练实例点拥有一个单元，所有训练实例点的单元构成特征空间的一个划分。最近邻将实例 $x_i$ 的类 $y_i$ 作为其单元中所有点的类标记（class label）。这样，每个单元的实例点的类别是确定的。

![](https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/20200706205102.png)

### 距离度量

特征空间中两个实例点的距离是两个实例点相似程序的反映。常用的距离有欧式距离，也可以是其他距离，如更一般的 $L_p$ 距离($L_p distance$)或 Minkowski距离（Minkowski distance）。

$x_i,x_j$ 的 $L_p$ 距离定义为：

$$L_p(x_i, x_j) = (\sum_{l=1}^{n} \vert x_{i}^{(l)} - x_{j}^{(l)} \vert ^{p})^{\frac{1}{p}}$$

当 $p=2$ 时候，称为欧式距离（Euclidean distance）。当 $p=2$ 时候，称为曼哈顿距离（Manhattan distance）。当 $p=\infty$ 时候，它是各个坐标距离的最大值。

### k值选择

k值选择会对k近邻法的结果产生重大影响。

如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，学习的近似误差（approximation error）会减小，只有与输入实例较近（相似的）训练实例才会对预测结果其作用。但缺点是学习的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小意味着整体模型变复杂，容易发生过拟合。

如果选择较大的k值，就相当于用较大领域中的训练实例进行预测。其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。k值增大就意味着整体的模型变得简单。

如果 $k=N$，无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过与简单，完全忽略训练实例中的大量有效信息，是不可取的。

在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选择最优的k值。

### 分类决策规则

k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。

多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为：

$$f: R^n \rightarrow \{ c_1,c_2,...,c_K \}$$

那么误分类的概率是：

$$P(Y \neq f(X)) = 1 - P(Y = f(X))$$

对给定的实例 $x \in X$，其最近邻的k个训练实例点构成集合 $N_k(x)$。如果涵盖 $N_k(x)$ 的区域的类别是 $c_j$，那么误分类率是：

$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq \c_j) = 1 - \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i = c_j)$$

要使误分类率最小即经验风险最小，就要使 $\sum_{x_i \in N_k(x)} I(y_i = c_j)$ 最大，所以多数表决规则等价于经验风险最小化。

## k近邻法的实现：kd树

实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。这点在特征空间的维数大以及训练数据容量大时尤其必要。

k近邻法最简单的实现方法使线性扫描（linear scan）。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法使不可行的。

为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。

### 构造kd树

kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分（partition）。构造kd树相当于不断用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应于一个k维超矩形区域。

构造kd树的方法如下：构造根节点，使根节点对应于k维空间中包含所有实例点的超矩形区域：通过下面的递归方法，不断地对k维空间进行切分生成子结点。在超矩形区域（节点）上选择一个坐标轴和此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子节点）；这时，实例被分到两个子区域，这个过程直到子区域内没有实例时终止（终止时的节点为叶节点）。在此过程中，将实例保存在相应的节点上。

通常，依次选择坐标轴对空间划分，选择训练实例点在选定坐标轴上的中位数（median）为切分点，这样得到的kd树是平衡的。注意，平衡的kd树在搜索时的效率未必使最优的。

**构造平衡kd树的算法**

输入：k维空间数据集 $T = {x_1,x_2,...,x_N}$

输出：kd树

- 开始：构造根节点，根节点对应于包含 $T$ 的k维空间的超矩形区域。选择 $x^{(1)}$ 为坐标轴，以 $T$ 中所有实例的 $x^{(1)}$ (特征向量的第一个维度)坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(1)}$ 垂直的超平面实现。
- 由根节点生成深度为1的左、右子结点；左子节点对应坐标 $x^{(1)}$ 小与切分点的子区域，右子结点对应于坐标 $x^{(1)}$ 大于切分点的子区域。将落在切分超平面上的实例点保存在根节点。
- 重复：对深度为 $j$ 的节点，选择 $x^{(l)}$ 为切分的坐标轴，其中 $l = j(mod k)+1$，以该节点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(l)}$ 垂直的超平面实现。由该节点生成深度为 $j+1$ 的左、右子结点：左子结点对应坐标 $x^{(l)}$ 小于切分点的子区域，右子节点对应坐标 $x^{(l)}$ 大于切分点的子区域。将落在切分超平面上的实例点保存在该节点。
- 直至两个子区域没有实例存在时停止，从而形成kd树的区域划分。

![特征空间划分](https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/20200707165045.png)

### 搜索kd树

利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。

![kd树示例](https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/20200707165122.png)

给定一个目标点，搜索其最近邻。首先找到包含目标点的叶节点；然后从该叶节点出发，依次回退到父节点；不断查找与目标点最邻近的节点，当确定不可能存在更近的节点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。

包含目标点的叶节点对应包含目标点的最小超矩形区域。以此叶节点的实例点作为当前最近点。目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部。然后返回当前节点的父节点，如果父节点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。如果存在这样的点，将此点作为新的当前最近点。算法转到更上一级的父节点，继续上述过程。如果父节点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。

**用kd树的最近邻搜索的算法：**

输入：已构造的kd树；目标点 $x$；

输出：$x$ 的最近邻

- 在kd树中找出包含目标点 $x$ 的叶节点：从根节点除法，递归地向下访问kd树。若目标点 $x$ 当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，直到子节点为叶节点为止。
- 以此叶节点为“当前最近点”。
- 递归地向上回退，在每个节点进行以下操作：
    - 如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”
    - 当前最近点一定存在于该节点一个子结点对应的区域。检查该子节点的父节的另一子节点对应的区域是有更近的点。具体地，检查另一子节点对应的区域是否以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一子节点对应的区域内存在距目标点更近的点，移动到另一子节点。接着，递归地进行最近邻搜索。如果不相交，向上回退。
- 当回退到根节点时，搜索结束。最后的“当前最近点”即为 $x$ 的最近邻点。

如果实例点使随机分布的，kd树搜索的平均计算复杂度是 $O(log N)$，这里 $N$ 是训练实例树。kd树更适用于训练实例数远大于空间维数时的k近邻搜索。当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线扫描。

## 代码实现

[kd树代码实现](https://github.com/hengxinCheung/StatisticalLearning/blob/master/knn/kdtree.py)