感知机（perceptron）是一个二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取 +1 和 -1 二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面。为此，倒入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。

## 感知机模型

假设输入空间（特征空间）是 $X \subseteq R^n$，输出空间是 $Y = {+1, -1}$。由输入空间到输出空间的如下函数称为感知机：

$$f(x) = sign(w \cdot x + b)$$

其中，$w$ 和 $b$ 为感知机模型参数。$w$ 叫做权值（weight）或权值向量（weight vecotr)，$b$ 叫做偏置（bias）。$w \cdot x$ 表示 $w$ 和 $x$ 的内积，$sign$ 是符号函数，即：

$$sign(x)=\begin{cases}
1, &x \ge 0 \\
0, &x \lt 0 \\
\end{cases}$$

感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合 $\{f | f(x)=w \cdot x + b\}$。

感知机有如下几何解释：线性方程

$$w \cdot x + b = 0$$

对应于特征空间 $R^n$ 中的一个超平面 $S$，其中 $w$ 是超平面的法向量，$b$ 是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类。因此，超平面 $S$ 也被称为分离超平面（separating hyperplane）。

![感知机模型](https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/20200705174302.png)

## 感知机学习策略

### 数据集的线性可分性

给定一个数据集，如果存在某个超平面 $S$：

$$w \cdot x + b = 0$$

能够将数据集正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_i = +1$ 的实例 $i$,有 $w \cdot x_i + b > 0$；对有$y_i = -11$ 的实例 $i$,有 $w \cdot x_i + b < 0$。则称数据集为线性可分数据集（linearly separable data set）；否则，称数据集线性不可分。

### 感知机学习策略

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数 $w$,$b$，需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。

损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 $w$,$b$ 的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面 $S$ 的总距离，这是感知机锁采用的。为此，首先写出输入空间任一点到超平面的距离：

$$\frac{1}{\vert\vert w \vert\vert} \vert w \cdot x_0 + b \vert$$

这里的 $\vert\vert w \vert\vert$ 是 $w$ 的 $L_2$ 范数。

其次，对于误分类的数据 $(x_i,y_i)$ 来说，

$$-y_i (w \cdot x_i + b) > 0$$

成立。因为当 $w \cdot x_i + b > 0$ 时，$y_i = -1$，而当 $w \cdot x_i + b < 0$ 时，$y_i = +1$。因此，误分类点 $x_i$ 到超平面 $S$ 的距离是：

$$-\frac{1}{\vert\vert w \vert\vert} \sum_{x_i \in M} y_i(w \cdot x + b)$$

不考虑 $vert\vert w \vert\vert$，就得到感知机学习的损失函数。

感知机 $sign(w \cdot x + b)$ 学习的损失函数定义为：

$$L(w, b) = -\sum_{x_i \in M} y_i(w \cdot x_i + b)$$

其中 $M$ 为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。

显然，损失函数 $L(w, b)$ 是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。

## 感知机学习算法

感知机学习问题转化为求解损失函数的最优化问题，最优化的方法是随机梯度下降法。

### 感知机学习算法的原始形式

求解参数 $w$,$b$，使其为以下损失函数极小化问题的解：

$$\underset{w,b}{min} L(w, b) = -\sum_{x_i \in M} y_i (w \cdot x_i + b)$$

感知机学习算法是误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）。首先，任意选取一个超平面 $w_0$,$b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程中不是一次使 $M$ 中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

假设误分类点集合 $M$ 是固定的，那么损失函数 $L(w, b)$ 的梯度由下面的公式给出：

$$\nabla_{w} L(w, b) = -\sum_{x_i \in M}y_ix_i$$

$$\nabla_{b} L(w, b) = -\sum_{x_i \in M}y_ix_i$$

随机选取一个误分类点 $(x_i, y_i)$，对 $w$,$b$ 进行更新：

$$w \leftarrow w + \eta y_i x_i$$

$$b \leftarrow b + \eta y_i$$

$\eta (0 \lt \eta \leq 1)$ 是步长，在统计学习中又称为学习率（learning rate）。这样，通过迭代可以期待损失函数 $L(w,b)$ 不断减小，直到为0。

**感知机学习算法的原始形式：**

输入：训练数据集 $T = \{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$；学习率 $\eta (0 \lt \eta \leq 1)$。

输出：$w,b$；感知机模型 $f(x)=sign(w \cdot x + b)$

- 选取初始值 $w_0,b_0$
- 在训练集中选取数据 $(x_i, y_i)$
- 如果 $y_i(w \cdot x_i + b) \leq 0$: $$w \leftarrow w + \eta y_i x_i \\ b \leftarrow b + \eta y_i$$
- 转至第二步，直至训练集中没有误分类点

注意：感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。

### 算法的收敛性

对于线性可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练集完全正确划分的分离超平面及感知机模型。

### 感知机学习算法的对偶形式

对偶形式的基本想法是，将 $w$ 和 $b$ 表示为实例 $x_i$ 和标记 $y_i$ 的线性组合的形式，通过求解其系数而求得 $w$ 和 $b$。不失一般性，可假设初始值 $w_0,b_0$ 均为0.对于误分类点 $(x_i, y_i)$ 通过

$$w \leftarrow w + \eta y_i x_i$$

$$b \leftarrow b + \eta y_i$$

逐步修改 $w,b$，设修改 $n$ 此，则 $w,b$ 关于 $(x_i, y_i)$ 的增量分别是 $\alpha_i y_i x_i$ 和 $\alpha_i y_i$，这里 $\alpha_i = n_i \eta$。这样，从学习过程不难看出，最后学习到的 $w,b$ 可分别表示为：

$$w = \sum_{i=1}^{N}\alpha_i y_i x_i$$

$$b = \sum_{i=1}^{N}\alpha_i y_i$$

实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分离。换句话说，这样的实例对学习结果影响最大。

**感知机学习算法的对偶形式：**

输入：训练数据集 $T = \{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$；学习率 $\eta (0 \lt \eta \leq 1)$。

输出：$\alpha, b$；感知机模型 $f(x)=sign(\sum_{j=1}^{N} \alpha_j y_j x_j \cdot x + b)$

其中 $\alpha = (\alpha_1, \alpha_2,...,\alpha_N)^{T}$

- $\alpha \leftarrow 0, b \leftarrow 0$
- 在训练集中选取数据 $(x_i, y_i)$
- 如果 $y_i(\sum_{j=1}^{N} \alpha_j y_j x_j \cdot x_j +b) \leq 0$，$$\alpha_i \leftarrow \alpha_i + \eta \\ b \leftarrow b + \eta y_i$$
- 转至第二步直到没有误分类数据。

对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例键的内积计算出来并以矩阵的形式存储，这个矩阵就是所有的Gram矩阵：

$$G = [x_i \cdot x_j]_{N \times N}$$
