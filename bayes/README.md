朴素贝叶斯（naive Bayes）法是基于贝叶斯定理于事务条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

## 朴素贝叶斯法的学习与分类

### 基本方法

设 $X$ 是定义在输入空间的随机向量，$Y$ 是定义在输出空间上的随机变量。$P(X, Y)$ 是 $X$ 和 $Y$ 的联合概率分布。训练数据集：

$$T = \{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \}$$

由 $P(X, Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习概率分布 $P(X, Y)$。具体地，学习以下先验概率分布以及条件概率分布。先验概率分布：

$$P(Y = c_k)$$

条件概率分布：

$$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)} | Y=c_k)$$

于是学习到联合概率分布 $P(X, Y)$。

条件概率分布 $P(X=x | Y=c_k)$ 由指数级数量的参数，其估计实际是不可行的。

朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：

$$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)} | Y=c_k) = \prod_{j=1}^{n} P(X^{(j)}=x^{(j)} | Y=c_k)$$

朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。

朴素贝叶斯法分类时，对给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k | X=x)$，将后验概率最大的类作为 $x$ 的类输出。后验概率计算根据贝叶斯定理进行：

$$P(Y=c_k | X=x) = \frac{P(X=x | Y=c_k) P(Y=c_k)}{\sum_{k} P(X=x | Y=c_k) P(Y=c_k)}$$

结合条件独立型假设，就有：

$$P(Y=c_k | X=x) = \frac{P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)} | Y=c_k}{\sum_{k} P(Y=c_k) \prod_{j}P(X^{(j)}=x^{(j)} | Y=c_k)}$$

这是朴素贝叶斯分类的基本公式。于是，朴素贝叶斯分类器可以表示为：

$$P(Y=c_k | X=x) = {argmax}_{c_k} \frac{P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)} | Y=c_k}{\sum_{k} P(Y=c_k) \prod_{j}P(X^{(j)}=x^{(j)} | Y=c_k)}$$

由于分母对所有 $c_k$ 都是相同的，所以：

$$y = \underset{c_k}{argmax} P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)} | Y=c_k)$$

### 后验概率最大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化。假设选择0-1损失函数，期望风险函数为：

$$R_{exp}(f) = E[L(Y, f(X))]$$

期望是对联合分布 $P(X, Y)$ 取的，由此取条件期望：

$$R_{exp}(f) = E_X \sum_{k=1}^{K}[L(c_k, f(X))] P(c_k | X)$$

为了使得期望风险最小化，只需对 $X=x$ 逐个极小化，由此得到：

$$f(x) = argmin \sum_{k=1}^{K} L(c_k, y)P(c_k | X=x) \\ = argmin \sun_{k=1}^{K} P(y \neq c_k | X=x) \\ = argmin (1-P(y=c_k | X=x)) \\ = argmax P(y=c_k | X=x)$$

这样一来，根据期望风险最小化准则就得到了后验概率最大化准则，即朴素贝叶斯法所采用的原理。朴素贝叶斯（naive Bayes）法是基于贝叶斯定理于事务条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

## 朴素贝叶斯法的学习与分类

### 基本方法

设 $X$ 是定义在输入空间的随机向量，$Y$ 是定义在输出空间上的随机变量。$P(X, Y)$ 是 $X$ 和 $Y$ 的联合概率分布。训练数据集：

$$T = \{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \}$$

由 $P(X, Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习概率分布 $P(X, Y)$。具体地，学习以下先验概率分布以及条件概率分布。先验概率分布：

$$P(Y = c_k)$$

条件概率分布：

$$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)} | Y=c_k)$$

于是学习到联合概率分布 $P(X, Y)$。

条件概率分布 $P(X=x | Y=c_k)$ 由指数级数量的参数，其估计实际是不可行的。

朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：

$$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)} | Y=c_k) = \prod_{j=1}^{n} P(X^{(j)}=x^{(j)} | Y=c_k)$$

朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。

朴素贝叶斯法分类时，对给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k | X=x)$，将后验概率最大的类作为 $x$ 的类输出。后验概率计算根据贝叶斯定理进行：

$$P(Y=c_k | X=x) = \frac{P(X=x | Y=c_k) P(Y=c_k)}{\sum_{k} P(X=x | Y=c_k) P(Y=c_k)}$$

结合条件独立型假设，就有：

$$P(Y=c_k | X=x) = \frac{P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)} | Y=c_k}{\sum_{k} P(Y=c_k) \prod_{j}P(X^{(j)}=x^{(j)} | Y=c_k)}$$

这是朴素贝叶斯分类的基本公式。于是，朴素贝叶斯分类器可以表示为：

$$P(Y=c_k | X=x) = {argmax}_{c_k} \frac{P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)} | Y=c_k}{\sum_{k} P(Y=c_k) \prod_{j}P(X^{(j)}=x^{(j)} | Y=c_k)}$$

由于分母对所有 $c_k$ 都是相同的，所以：

$$y = \underset{c_k}{argmax} P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)} | Y=c_k)$$

### 后验概率最大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化。假设选择0-1损失函数，期望风险函数为：

$$R_{exp}(f) = E[L(Y, f(X))]$$

期望是对联合分布 $P(X, Y)$ 取的，由此取条件期望：

$$R_{exp}(f) = E_X \sum_{k=1}^{K}[L(c_k, f(X))] P(c_k | X)$$

为了使得期望风险最小化，只需对 $X=x$ 逐个极小化，由此得到：

$$f(x) = argmin \sum_{k=1}^{K} L(c_k, y)P(c_k | X=x) \\ = argmin \sum_{k=1}^{K} P(y \neq c_k | X=x) \\  = argmin (1-P(y=c_k | X=x))  \\ = argmax P(y=c_k | X=x)$$

这样一来，根据期望风险最小化准则就得到了后验概率最大化准则，即朴素贝叶斯法所采用的原理。

## 朴素贝叶斯法的参数估计

### 极大似然估计

在朴素贝叶斯法中，学习意味着估计 $P(Y=c_k)$ 和 $P(X^{(j)}=x^{(j)} | Y=c_k)$。可以应用极大似然估计法估计相应的概率。先验概率 $P(Y=c_k)$ 的极大似然估计是：

$$P(Y=c_k) = \frac{\sum_{i=1}^{N} I(y_i=c_k)}{N}$$

设第 $j$ 个特征 $x_^{(j)}$ 可能取值的集合为 $\{ a_{j1}, a_{j2}, ..., a_{jS_j} \}$，条件概率 $P(X^{j}=a_{jl}=c_k)$ 的极大似然估计是：

$$P(X^{j}=a_{jl} | Y=c_k) = \frac{\sum_{i=l}^{N} I(x_i^{(j)}=a_{jl}, y_j=c_k)}{\sum_{j=1}^{N}I(y_i=c_k)}$$

### 学习与分类算法

**朴素贝叶斯算法（navie Bayes algorithm）**

输入：训练数据 $T={(x_1,y_1), (x_2, y_2), ..., (x_N, y_N)}$，其中 $x_i = (x_i^{1}, x_i^{2}, ..., x_i^{n})^T$，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$x_i^(j) \in 
\{ a_{j1}, a_{j2}, ..., a_{jS_j} \}$，$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值；实例 $x$；

输出：实例 $x$ 的分类

- 计算先验概率和条件概率 $$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N} \\ P(X^{(j)}=a_{jl} \vert Y=c_k) = \frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}$$
- 对于给定的实例 $x=(x^{(1)}, x^{(2)}, ..., x^{(n)})^T$，计算 $$P(Y=c_k) \prod_{j=1}^{n} P(X^{(j)}=x^{(j)} \vert Y=c_k)$$
- 确定实例 $x$ 的类: $$y=\underset{c_k}{argmax} P(Y=c_k) \prod_{j=1}^{n} P(X^{(j)}=x^{(j)} \vert Y=c_k)$$

## 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体第，条件概率的贝叶斯估计是：

$$P_{\lambda}(X^{(j)}=a_{jl} \vert Y=c_k) = \frac{\sum_{i=1}^{N} I(x_i^{(j)=a_{jl},y_i=c_k}) + \lambda}{\sum_{i=1}^{N} I(y_i=c_k) + S_j \lambda}$$

式中 $\lambda \geq 0$。等价于在随机变量各个取值的频数上赋予一个整数。当 $\lambda = 0$ 时候就是极大似然估计。常取 $\lambda = 1$，这时称为拉普拉斯平滑（Laplace smoothing）。显然，对任何 $l=1,2,..,S, k=1,2,...,K$，有：

$$P_{\lambda}(X^{(j)}=a_{jl} \vert Y=c_k) \gt 0$$

$$\sum_{l=1}^{S_j}P(X^{(j)}=a_{jl} \vert Y=c_k) = 1$$

同样，先验概率的贝叶斯估计是：

$$P_{\lambda}(Y=c_k) = \frac{\sum_{i=1}^{N} I(y_i=c_k) + \lambda}{N + K\lambda}$$

## 实现代码

[实现代码](https://github.com/hengxinCheung/StatisticalLearning/blob/master/bayes/naviebayes.py)